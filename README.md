# ğŸ™ï¸ Comparative Study on LSTM Networks for STT Conversion  
### ğŸ§  Research Project: Variations in Attention Mechanisms & Loss Functions

This repository implements an **LSTM-based Speech-to-Text (STT)** comparative study analyzing **attention mechanisms (Bahdanau, Multi-Head)** and **loss functions (CTC, Cross-Entropy)**.

---

## ğŸ§  Approach:
- **Baseline**: Encoder-decoder LSTM model.
- **Attention Variants**: Bahdanau, Multi-head (Transformer-inspired).
- **Loss Variants**: CTC loss vs Cross-Entropy.
- **Metrics**: Word Error Rate (WER), Character Error Rate (CER).

---

## ğŸ“‚ Dataset:
- **LibriSpeech ASR Corpus (train-clean-100)**  
[Download Here](https://www.openslr.org/12/)

Organize as:
```
data/
 â”œâ”€â”€ train/
 â”‚    â”œâ”€â”€ audio/
 â”‚    â””â”€â”€ transcripts/
 â””â”€â”€ test/
      â”œâ”€â”€ audio/
      â””â”€â”€ transcripts/
```

---

## ğŸš€ Getting Started:
1ï¸âƒ£ Clone the repository:
```bash
git clone https://github.com/your-username/lstm-attention-stt-comparison.git
cd lstm-attention-stt-comparison
```

2ï¸âƒ£ Install dependencies:
```bash
pip install -r requirements.txt
```

3ï¸âƒ£ Train models:
```bash
python src/train.py --model_type attention
```

4ï¸âƒ£ Evaluate:
```bash
python src/evaluate.py
```

5ï¸âƒ£ Inference:
```bash
python src/inference.py
```

---

## ğŸ“Š Highlights:
- Comparative study of **attention vs no-attention models**.
- Implements **CTC and Cross-Entropy loss functions**.
- Visualizable **attention weights** for interpretability.

---

## ğŸ“œ License:
MIT License Â© 2025
